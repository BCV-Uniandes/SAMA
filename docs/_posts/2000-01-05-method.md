---
title: "Method"
bg: purple
color: black
fa-icon: star
---

#### Method
  * Our method processes information in two parallel branches, one for each data modality.
  * For the visual information, we use the backbone proposed by [Daza et al.](https://link.springer.com/chapter/10.1007/978-3-030-60946-7_12) producing a visual representation (*V*).
  * The clinical information goes through an MLP outputting a clinical	representation (*C*).
  * We process the two branches' output using our SAMA module

  ![SAMA Block](./img/SAMA.png)

##### Spatially Aware Attention
  * We generate dynamic filters [(Li et.al, 2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Tracking_by_Natural_CVPR_2017_paper.pdf) using a dynamic convolutional layer, which produces new filters depending on the patientsâ€™ clinical	information (embedded in the clinical representation).

  {:refdef: style="text-align: center;"}
  ![Dynamic Filters](./img/dynamic_filters.svg)
  <img src="./img/dynamic_filters.svg">
  {: refdef}
  * We generate a 3D spatial coordinate representation (*S*) of the visual information
  * We calculate a dynamic response map R as:

  {:refdef: style="text-align: center;"}
  ![Dynamic Filters](./img/response_maps.svg)
  <img src="./img/response_maps.svg">
  {: refdef}

##### Fusion with Channel Attention
 * We concatenate response maps, visual, spatial and clinical data
 * This concatenation goes through a Squeeze-and-Excitation 
	block (SE) [(J. Hu, et al., 2018)](https://arxiv.org/abs/1709.01507)
 * This weighted representation goes through a pointwise 
	convolutional layer to fuse all the multimodal information at each 
	location independently, producing the fused representation:

  {:refdef: style="text-align: center;"}
  ![Dynamic Filters](./img/final_o.svg)
  <img src="./img/final_o.svg">
  {: refdef}

* Finally, the output goes through a Multilayer perceptron (MLP) with one hidden layer
* The network outputs a predicted score representing the probability of having cancer